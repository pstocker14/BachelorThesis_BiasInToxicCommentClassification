# BA Thesis: Mitigating Bias in Toxic Comment Classification

**Author:** Philipp Stocker  
**Start date:** 2025-10-01  
**Created scaffold:** 2025-10-18  
**Project end date:** 2026-01-12

This repository contains code and materials for a bachelor's thesis on detecting and mitigating bias against LGBTQ identities in toxic comment classification using the Jigsaw *Unintended Bias in Toxicity Classification* dataset.

## Table of Contents

- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Project Structure](#project-structure)
- [Data](#data)
- [Usage](#usage)
- [Reproducibility](#reproducibility)
- [License](#license)

## Prerequisites

- Python 3.9 or higher
- Conda (for environment management)
- Git

## Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/pstocker14/BachelorThesis_BiasInToxicCommentClassification.git
   cd BachelorThesis_BiasInToxicCommentClassification
   ```

2. **Set up the Conda environment:**
   ```bash
   # Create a new conda environment
   conda create -n bias_mitigation_env python=3.9

   # Activate the environment
   conda activate bias_mitigation_env

   # Install dependencies
   pip install -r requirements.txt
   ```

## Project Structure

```
BachelorThesis_BiasInToxicCommentClassification/
├── data/
│   ├── raw/          # Unaltered source data (read-only)
│   └── processed/    # Cleaned/derived datasets for modeling
├── models/           # Trained models and vectorizers
├── notebooks/        # Jupyter notebooks for analysis, training, and evaluation
├── results/          # Files containing the bias and performance results of evry model
├── src/              # Reusable Python modules
├── requirements.txt  # Python dependencies
├── README.md         # This file
└── .gitignore        # Ignore patterns
```

## Data

The project uses the Jigsaw *Unintended Bias in Toxicity Classification* dataset. The raw data should be placed in `data/raw/`. Due to size and licensing, the data is not included in this repository.

- Download the dataset from [Kaggle](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data).
- Place the files in `data/raw/`.

Processed data is generated by running the preprocessing notebooks.

## Usage

1. Ensure the environment is activated: `conda activate bias_mitigation_env`
2. Run Jupyter notebooks in the `notebooks/` directory for data exploration, preprocessing, training, and evaluation.
3. The notebooks call different functions from python-files in `src/` which can be adapted via different parameter settings.
4. Each model/mitigation technique has its own notebook including various cells for both performance and fairness evaluations.
   Cross evaluations can either be done separately or via the plots generated in `print_final_thesis_plots.ipynb`.

Example to run a notebook:
```bash
jupyter notebook notebooks/data_exploration_and_analysis.ipynb
```

## Reproducibility

To reproduce the results:

1. Follow the installation steps above.
2. Download and place the dataset as described in [Data](#data).
3. Run the notebooks in order:
   - `data_exploration_and_analysis.ipynb`
   - `data_preprocessing_and_preparation.ipynb`
   - Training notebooks (e.g., `base_model_training_and_evaluation.ipynb`, etc.)
   - `print_final_thesis_plots.ipynb` for final plots (optional).

All random seeds are set for reproducibility. Results are saved in `results/` and models in `models/`.

## License

This project is for academic purposes.

